


You are an expert full-stack systems architect + backend engineer. Your job: **review and implement a complete, runnable, tested, production-ready codebase** for this platform that fully implements the five layers: Market, Agent, Client, Property, Behavior — exactly following the specification I provided. 

You must produce working code and automated verification that **proves** everything exists and works. Do not stop until **all automated checks pass**.

## REQUIRED STACK & FORMAT

* Language: **Python 3.11+**
* Web framework: **FastAPI** (with OpenAPI/Swagger)
* Database: **PostgreSQL** (use SQL scripts + Alembic migrations)
* Optional vector support: **pgvector** (include as optional module)
* Background workers: **Celery + Redis** or **RQ + Redis** for behavior processing (choose one and implement)
* Tests: **pytest** with automated test suite that runs locally
* Container: **Dockerfile** and **docker-compose.yml** to run Postgres, Redis, web, worker
* Repo deliverable: full folder structure with README and `make` or `scripts/` to run all checks

## MANDATORY DELIVERABLES

1. **Codebase** with modules per layer:

   * `core/` (shared utilities, config, logging, auth stubs)
   * `market/` (models, CRUD, analytics)
   * `agent/` (models, CRUD, ranking)
   * `client/` (models, CRUD, qualification)
   * `property/` (models, CRUD, recommendation)
   * `behavior/` (event ingestion, analytics pipeline)
   * `realestate_gpt/` (retriever + stubbed generator + learning hook)
   * `api/` (FastAPI app, routers for each layer)
   * `migrations/` (Alembic)
   * `tests/` (unit + integration)
   * `docs/` (architecture, ERD image or ascii, sequence diagrams)
2. **SQL schema** (PostgreSQL) created via Alembic migrations — not just a SQL file.
3. **Automated verification script** `scripts/verify_all.sh` (or `Makefile` target) that:

   * boots docker-compose (Postgres + Redis), runs migrations
   * seeds sample data
   * runs test suite (`pytest`)
   * runs an endpoint smoke test that validates required endpoints
   * validates DB schema (tables/columns/PKs/FKs/indexes present)
   * runs behavior analytics job and asserts expected outputs
   * runs `realestate_gpt` sample query and asserts a structured response
4. **API endpoints** for each layer (must exist and be documented in OpenAPI):

   * For each layer: `POST /<layer>/ingest`, `GET /<layer>/query`, `GET /<layer>/predict`, `GET /<layer>/recommend` (if applicable)
   * Additional endpoints: `POST /behavior/event`, `GET /market/heatmap`, `GET /agent/rankings`, `POST /gpt/respond`
5. **Intelligence functions** implemented (at minimum as working code + tests):

   * `client_qualification_score(client_id)` → numeric [0..1]
   * `market_heatmap()` → list of areas with heat score
   * `behavior_trigger_analysis()` → returns top N trigger words with lift
   * `closing_probability(agent_id, client_id, property_id)` → probability
   * `recommend_properties(client_id)` → ordered list of property_ids with reasons
   * `agent_skill_ranking()` → leaderboard
6. **Behavioral engine**:

   * Ingest events via API into `behavior_events` table
   * Background job(s) aggregate events, compute triggers, update pattern groups
   * Provide an endpoint to fetch discovered triggers/patterns
7. **Real Estate GPT layer**:

   * Implement a `retriever()` that searches relevant DB fields (or embeddings if enabled)
   * Implement `realestate_gpt.respond(client_input, agent_context)` that composes a reasoned reply using data from all 5 layers
   * The function must log which layers and records it used to create the reply (for auditability)
8. **Indexing & Performance**:

   * All heavy query columns must have proper indexes (demonstrate in SQL)
   * Include partitioning strategy or sharding notes in docs
9. **Security & Validation**:

   * Basic input validation
   * Rate limiting stub (middleware) with config
   * Authentication stub (API key or JWT) gating ingestion endpoints
10. **Observability**:

    * Request logging
    * Metrics endpoint or Prometheus metrics stub
    * Error reporting hooks (Sentry stub)
11. **Documentation**:

    * `README.md` with exact commands to run locally
    * `docs/ERD.png` or `.svg` (or ASCII if images not possible)
    * Design notes describing how data flows from ingestion → behavior → GPT
12. **Postman / OpenAPI collection** included for testing endpoints

## SCHEMA & TABLE REQUIREMENTS (exact names required)

Create tables with these names and required columns (add more if needed):

### market_insights

* `area_id` (PK, uuid)
* `area_name` (text)
* `price_avg` (numeric)
* `demand_score` (numeric)
* `supply_score` (numeric)
* `sales_volume` (bigint)
* `new_projects` (jsonb)
* `agent_density` (numeric)
* `updated_at` (timestamptz)

### agent_activity

* `agent_id` (PK, uuid)
* `name` (text)
* `calls_per_day` (int)
* `lead_responses` (int)
* `objections_heard` (jsonb)
* `deals_closed` (int)
* `projects_assigned` (jsonb)
* `closing_rate` (numeric)
* `avg_response_time` (numeric)
* `avg_closing_price` (numeric)
* `skill_scores` (jsonb)
* `updated_at` (timestamptz)

### client_profiles

* `client_id` (PK, uuid)
* `name` (text)
* `interest` (jsonb)
* `budget` (numeric)
* `location_targets` (jsonb)
* `objections` (jsonb)
* `decision_speed` (text)
* `emotional_type` (text)
* `qualification_score` (numeric)
* `response_history` (jsonb)
* `updated_at` (timestamptz)

### properties

* `project_id` (uuid)
* `unit_id` (uuid)
* `title` (text)
* `price` (numeric)
* `payment_plan` (jsonb)
* `cash_ratio` (numeric)
* `delivery_date` (date)
* `amenities` (jsonb)
* `real_demand` (numeric)
* `sales_velocity` (numeric)
* `negative_reviews` (jsonb)
* `positive_triggers` (jsonb)
* `updated_at` (timestamptz)
* Primary key should be composite (`project_id`, `unit_id`)

### behavior_events

* `event_id` (PK, uuid)
* `agent_id` (uuid) (FK -> agent_activity.agent_id)
* `client_id` (uuid) (FK -> client_profiles.client_id)
* `property_project_id` (uuid)
* `property_unit_id` (uuid)
* `trigger_word` (text)
* `emotion_detected` (text)
* `response_speed` (numeric)
* `conversion_effect` (numeric)
* `time_of_event` (timestamptz)
* `pattern_group` (text)
* `metadata` (jsonb)
* `updated_at` (timestamptz)

> **Ensure foreign keys and indexes are present**. Provide DDL via Alembic migrations.

## API CONTRACT (must be implemented, return JSON)

* `POST /market/ingest` — ingest market snapshot (accepts JSON array)
* `GET  /market/heatmap` — returns areas ordered by heat
* `POST /agent/ingest` — ingest agent activity
* `GET  /agent/rankings` — returns ranking with reasons
* `POST /client/ingest` — ingest client profile / update
* `GET  /client/qualification/{client_id}` — returns qualification score + explanation
* `POST /property/ingest` — ingest property units
* `GET  /property/compare?unitA=...&unitB=...` — compare two units, return pros/cons
* `POST /behavior/event` — ingest single behavior event (must enqueue processing)
* `GET  /behavior/triggers` — returns top triggers and lift scores
* `POST /gpt/respond` — payload: `{client_input, agent_id, client_id, property}` → returns `{text, reasoning, used_records:[{layer,ids}], confidence}`

All endpoints must validate inputs and return structured errors.

## AUTOMATED VERIFICATION (agent MUST implement)

Implement `scripts/verify_all.sh` (or Python script `scripts/verify_all.py`) that performs the following **automatically** and exits non-zero on failure:

1. Start services: `docker-compose up --build -d`
2. Wait for Postgres & Redis healthy
3. Run Alembic migrations
4. Run DB schema checks:

   * Assert all required tables exist
   * Assert required columns exist with expected types
   * Assert PKs, FKs, and at least one useful index per heavy column
5. Seed the DB with **representative sample data** (at least 10 agents, 50 clients, 100 properties, 1000 behavior events)
6. Run unit & integration tests: `pytest -q` (must pass)
7. Call each required API endpoint (ingest + query + predict + recommend) and assert valid responses
8. Trigger behavior analytics worker job and assert it computed triggers and updated `pattern_group` for at least 10 events
9. Call `POST /gpt/respond` with a realistic scenario — assert response contains `used_records` from at least 3 layers and a `confidence` numeric field
10. Produce a final summary JSON file `build_report.json` containing results of each check

If any step fails, the script must print a clear failure message and fail the run.

## TEST CASES YOU MUST WRITE

* DB schema unit tests: check presence & types of columns, FK constraints.
* Endpoint tests:

  * Ingest market snapshot → read back correct aggregated `price_avg` for an area.
  * Ingest agent activity → `GET /agent/rankings` includes the ingested agent.
  * Ingest client profile → `GET /client/qualification/{id}` returns score between 0 and 1.
  * Ingest property → `GET /property/compare` returns non-empty reasons.
  * Ingest behavior events → after worker runs, `GET /behavior/triggers` returns top triggers including expected words.
  * `POST /gpt/respond` returns `used_records` containing IDs from market/agent/client.

## QUALITY RULES (must be followed)

* All code must be **type hinted** and pass `mypy` checks (include `pyproject.toml`)
* Lint with `ruff` or `flake8` (include config)
* Tests must be deterministic — use seeded randomness where needed
* Use environment variables for configuration; do not hardcode secrets
* Include `docker-compose.override.yml` for local development conveniences
* Include `CI` script (`.github/workflows/ci.yml`) that runs `scripts/verify_all.sh` in CI

## OPTIONAL BUT HIGHLY RECOMMENDED

* Implement embeddings + vector search (pgvector) and include as optional flow; tests should skip if pgvector not present.
* Provide example Jupyter notebook `notebooks/demo.ipynb` showcasing:

  * heatmap generation
  * recommend_properties output
  * behavior_trigger_analysis visual

## DELIVERY FORMAT

* The agent must output a **zip archive** of the repository or commit to the Replit project with all files.
* The `scripts/verify_all.sh` must run inside the project root and produce `build_report.json`.

## FINAL CHECK: DEPLOYABLE DEMO

After all checks pass, produce a final section in the repo `docs/DEMO.md` that contains:

* curl commands to reproduce the demo
* a short recorded demo script (text) showing how data flows and how GPT composes a reply
* explanation of scaling & production improvements (caching, RBAC, infra)

---

### LAST INSTRUCTION (enforcement)

You must **not** stop at code generation. You must:

1. Run migrations and tests locally inside the Replit environment.
2. Execute `scripts/verify_all.sh`.
3. Fix anything failing until the verification script exits `0`.
4. Commit all changes and produce the zip or final repo.

If any step cannot be completed due to environment limitations in Replit, produce a **diagnostic report** explaining exactly which steps could not run and provide fully reproducible commands and scripts so the project will pass verification on a real machine (the scripts themselves must still exist and be runnable). The diagnostic report is an acceptable fallback **only if** you couldn't run steps — but you must still generate all code, migrations, tests, scripts, and docs. 

